{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d863e40f",
   "metadata": {},
   "source": [
    "# Phần 1: Khám phá Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56f0e1",
   "metadata": {},
   "source": [
    "## Task 1.1: Tạo Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "409cc605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor từ list:\n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "Tensor từ NumPy array:\n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "Ones Tensor:\n",
      " tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "\n",
      "Random Tensor:\n",
      " tensor([[0.7972, 0.3290],\n",
      "        [0.3406, 0.5863]])\n",
      "\n",
      "Shape của tensor: torch.Size([2, 2])\n",
      "Datatype của tensor: torch.float32\n",
      "Device lưu trữ tensor: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Tạo tensor từ list\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(f\"Tensor từ list:\\n {x_data}\\n\")\n",
    "\n",
    "# Tạo tensor từ NumPy array\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(f\"Tensor từ NumPy array:\\n {x_np}\\n\")\n",
    "\n",
    "# Tạo tensor với các giá trị ngẫu nhiên hoặc hằng số\n",
    "x_ones = torch.ones_like(x_data) # tạo tensor gồm các số 1 có cùng shape với x_data\n",
    "print(f\"Ones Tensor:\\n {x_ones}\\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # tạo tensor ngẫu nhiên\n",
    "print(f\"Random Tensor:\\n {x_rand}\\n\")\n",
    "\n",
    "# In ra shape, dtype, và device của tensor\n",
    "print(f\"Shape của tensor: {x_rand.shape}\")\n",
    "print(f\"Datatype của tensor: {x_rand.dtype}\")\n",
    "print(f\"Device lưu trữ tensor: {x_rand.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8141b1e5",
   "metadata": {},
   "source": [
    "## Task 1.2: Các phép toán trên Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc0d3270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. x_data + x_data:\n",
      "tensor([[2, 4],\n",
      "        [6, 8]])\n",
      "\n",
      "2. x_data * 5:\n",
      "tensor([[ 5, 10],\n",
      "        [15, 20]])\n",
      "\n",
      "3. x_data @ x_data.T (Nhân ma trận):\n",
      "tensor([[ 5, 11],\n",
      "        [11, 25]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Cộng x_data với chính nó\n",
    "add_result = x_data + x_data\n",
    "print(f\"1. x_data + x_data:\\n{add_result}\\n\")\n",
    "\n",
    "# 2. Nhân x_data với 5\n",
    "mul_result = x_data * 5\n",
    "print(f\"2. x_data * 5:\\n{mul_result}\\n\")\n",
    "\n",
    "# 3. Nhân ma trận x_data với x_data.T\n",
    "matmul_result = x_data @ x_data.T\n",
    "print(f\"3. x_data @ x_data.T (Nhân ma trận):\\n{matmul_result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be4727b",
   "metadata": {},
   "source": [
    "## Task 1.3: Indexing và Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bacf256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Hàng đầu tiên: tensor([1, 2])\n",
      "\n",
      "2. Cột thứ hai: tensor([2, 4])\n",
      "\n",
      "3. Giá trị (hàng 2, cột 2): 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Lấy ra hàng đầu tiên (index 0)\n",
    "row_first = x_data[0] \n",
    "print(f\"1. Hàng đầu tiên: {row_first}\\n\")\n",
    "\n",
    "# 2. Lấy ra cột thứ hai (index 1)\n",
    "col_second = x_data[:, 1]\n",
    "print(f\"2. Cột thứ hai: {col_second}\\n\")\n",
    "\n",
    "# 3. Lấy ra giá trị ở hàng thứ hai (index 1), cột thứ hai (index 1)\n",
    "value_1_1 = x_data[1, 1]\n",
    "print(f\"3. Giá trị (hàng 2, cột 2): {value_1_1}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd6c52",
   "metadata": {},
   "source": [
    "## Task 1.4: Thay đổi hình dạng Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7af9614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor 4x4 ban đầu:\n",
      "tensor([[0.6515, 0.4333, 0.1578, 0.7543],\n",
      "        [0.7542, 0.1521, 0.4136, 0.1390],\n",
      "        [0.0415, 0.4913, 0.6765, 0.7910],\n",
      "        [0.3594, 0.6944, 0.1474, 0.7489]])\n",
      "\n",
      "Tensor 16x1 sau khi reshape:\n",
      "tensor([[0.6515],\n",
      "        [0.4333],\n",
      "        [0.1578],\n",
      "        [0.7543],\n",
      "        [0.7542],\n",
      "        [0.1521],\n",
      "        [0.4136],\n",
      "        [0.1390],\n",
      "        [0.0415],\n",
      "        [0.4913],\n",
      "        [0.6765],\n",
      "        [0.7910],\n",
      "        [0.3594],\n",
      "        [0.6944],\n",
      "        [0.1474],\n",
      "        [0.7489]])\n",
      "\n",
      "Shape cũ: torch.Size([4, 4])\n",
      "Shape mới: torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "# 1. Sử dụng torch.rand để tạo một tensor có shape (4, 4)\n",
    "tensor_4x4 = torch.rand(4, 4)\n",
    "print(f\"Tensor 4x4 ban đầu:\\n{tensor_4x4}\\n\")\n",
    "\n",
    "# 2. Dùng reshape để đổi thành (16, 1)\n",
    "tensor_16x1 = tensor_4x4.reshape(16, 1)\n",
    "print(f\"Tensor 16x1 sau khi reshape:\\n{tensor_16x1}\\n\")\n",
    "print(f\"Shape cũ: {tensor_4x4.shape}\")\n",
    "print(f\"Shape mới: {tensor_16x1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625aebc0",
   "metadata": {},
   "source": [
    "# Phần 2: Tự động tính Đạo hàm với autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e82a34c",
   "metadata": {},
   "source": [
    "## Task 2.1: Thực hành với autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "311d43db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1.], requires_grad=True)\n",
      "y: tensor([3.], grad_fn=<AddBackward0>)\n",
      "grad_fn của y: <AddBackward0 object at 0x75701731e9b0>\n",
      "Đạo hàm của z theo x: tensor([18.])\n"
     ]
    }
   ],
   "source": [
    "# Tạo một tensor và yêu cầu tính đạo hàm cho nó\n",
    "x = torch.ones(1, requires_grad=True)\n",
    "print(f\"x: {x}\")\n",
    "\n",
    "# Thực hiện một phép toán\n",
    "y = x + 2\n",
    "print(f\"y: {y}\")\n",
    "\n",
    "# y được tạo ra từ một phép toán có x, nên nó cũng có grad_fn\n",
    "print(f\"grad_fn của y: {y.grad_fn}\")\n",
    "\n",
    "# Thực hiện thêm các phép toán\n",
    "z = y * y * 3\n",
    "\n",
    "# Tính đạo hàm của z theo x\n",
    "z.backward() # tương đương z.backward(torch.tensor(1.))\n",
    "\n",
    "# Đạo hàm được lưu trong thuộc tính .grad\n",
    "# Ta có z = 3 * (x+2)^2 => dz/dx = 6 * (x+2). Với x=1, dz/dx = 18\n",
    "print(f\"Đạo hàm của z theo x: {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc20e7d",
   "metadata": {},
   "source": [
    "## **Câu hỏi:** Chuyện gì xảy ra nếu bạn gọi z.backward() một lần nữa? Tại sao?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89ae3978",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mz\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mĐạo hàm cấp 2 của z theo x: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.grad\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NLP/.venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NLP/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NLP/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(f\"Đạo hàm cấp 2 của z theo x: {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df77f0ca",
   "metadata": {},
   "source": [
    "Nếu gọi z.backward() một lần nữa thì chương trình báo lỗi vì lý do chính là để tiết kiệm bộ nhớ:\n",
    "\n",
    "*   Đồ thị Tính toán: Khi thực hiện các phép toán trên tensor có `requires_grad=True`, PyTorch sẽ âm thầm xây dựng một \"đồ thị tính toán\"\n",
    "\n",
    "*   Quá trình `backward()`: Khi gọi `z.backward()`, PyTorch sử dụng đồ thị này để đi ngược lại (từ z về x), áp dụng quy tắc chuỗi để tính đạo hàm $\\frac{dz}{dx}$ và lưu kết quả vào `x.grad`\n",
    "\n",
    "*   Xóa Đồ thị: Theo mặc định, ngay sau khi `z.backward()` tính toán xong và lưu gradient, PyTorch sẽ xóa đồ thị tính toán đó đi. Việc này giải phóng bộ nhớ đã dùng để lưu các giá trị trung gian cần thiết cho việc tính đạo hàm\n",
    "\n",
    "*   Lỗi ở lần gọi thứ hai: Khi gọi `z.backward()` lần thứ hai, PyTorch cố gắng tìm lại đồ thị để tính toán đạo hàm, nhưng đồ thị không còn tồn tại nữa. Do đó, chương trình bị lỗi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7da8e4",
   "metadata": {},
   "source": [
    "# Phần 3: Xây dựng Mô hình đầu tiên với torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722028f8",
   "metadata": {},
   "source": [
    "## Task 3.1: Lớp nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e46037e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 5])\n",
      "Output shape: torch.Size([3, 2])\n",
      "Output:\n",
      " tensor([[-0.9573,  0.5969],\n",
      "        [-0.1251, -0.5139],\n",
      "        [-0.6276,  0.2310]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Khởi tạo một lớp Linear biến đổi từ 5 chiều -> 2 chiều\n",
    "linear_layer = torch.nn.Linear(in_features=5, out_features=2)\n",
    "\n",
    "# Tạo một tensor đầu vào mẫu\n",
    "input_tensor = torch.randn(3, 5) # 3 mẫu, mỗi mẫu 5 chiều\n",
    "\n",
    "# Truyền đầu vào qua lớp linear\n",
    "output = linear_layer(input_tensor)\n",
    "\n",
    "print(f\"Input shape: {input_tensor.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output:\\n {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb764ba7",
   "metadata": {},
   "source": [
    "## Task 3.2: Lớp nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e363686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4])\n",
      "Output shape: torch.Size([4, 3])\n",
      "Embeddings:\n",
      " tensor([[ 0.4616,  0.1849, -0.4434],\n",
      "        [-1.7394, -0.4961, -1.0500],\n",
      "        [ 1.1849,  0.1603, -2.3531],\n",
      "        [-0.0363,  0.8099, -0.6961]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Khởi tạo lớp Embedding cho một từ điển 10 từ, mỗi từ biểu diễn bằng vector 3 chiều\n",
    "embedding_layer = torch.nn.Embedding(num_embeddings=10, embedding_dim=3)\n",
    "\n",
    "# Tạo một tensor đầu vào chứa các chỉ số của từ (ví dụ: một câu)\n",
    "# Các chỉ số phải nhỏ hơn 10\n",
    "input_indices = torch.LongTensor([1, 5, 0, 8])\n",
    "\n",
    "# Lấy ra các vector embedding tương ứng\n",
    "embeddings = embedding_layer(input_indices)\n",
    "\n",
    "print(f\"Input shape: {input_indices.shape}\")\n",
    "print(f\"Output shape: {embeddings.shape}\")\n",
    "print(f\"Embeddings:\\n {embeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35347444",
   "metadata": {},
   "source": [
    "## Task 3.3: Kết hợp thành một nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b44eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape: torch.Size([1, 4, 2])\n",
      "Model output: tensor([[[-0.4086, -0.1009],\n",
      "         [-0.2795, -0.2178],\n",
      "         [-0.6725, -0.0585],\n",
      "         [-0.4884, -0.3320]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MyFirstModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(MyFirstModel, self).__init__()\n",
    "        # Định nghĩa các lớp (layer) bạn sẽ dùng\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU() # Hàm kích hoạt\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, indices):\n",
    "        # Định nghĩa luồng dữ liệu đi qua các lớp\n",
    "        # 1. Lấy embedding\n",
    "        embeds = self.embedding(indices)\n",
    "        # 2. Truyền qua lớp linear và hàm kích hoạt\n",
    "        hidden = self.activation(self.linear(embeds))\n",
    "        # 3. Truyền qua lớp output\n",
    "        output = self.output_layer(hidden)\n",
    "        return output\n",
    "    \n",
    "# Khởi tạo và kiểm tra mô hình\n",
    "model = MyFirstModel(vocab_size=100, embedding_dim=16, hidden_dim=8, output_dim=2)\n",
    "input_data = torch.LongTensor([[1, 2, 5, 9]]) # một câu gồm 4 từ\n",
    "\n",
    "output_data = model(input_data)\n",
    "print(f\"Model output shape: {output_data.shape}\")\n",
    "print(f\"Model output: {output_data}\")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
